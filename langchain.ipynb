{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a626261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnablePassthroughWithInput\n",
    "from langchain.llms import HuggingFaceHub, huggingface_endpoint\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa34172",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set token as env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279cb794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_31724\\3287684676.py:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "e:\\langchain-intro\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\", \n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f405d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92772fd8",
   "metadata": {},
   "source": [
    "## Basic Inference Using Huggin Face and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaefbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Load locally (make sure you have enough GPU/CPU resources)\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Wrap it in LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Use in LangChain\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1dcdb",
   "metadata": {},
   "source": [
    "## Prompt Template and LLM Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed92d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for Indian. Suggest fancy name for this'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine}. Suggest fancy name for this\"\n",
    ")\n",
    "\n",
    "prompt_template.format(cuisine='Indian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cbca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7940\\3993948497.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7940\\3993948497.py:8: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(cuisine='Indian')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Indian restaurant'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt_template\n",
    ")\n",
    "\n",
    "chain.run(cuisine='Indian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32147ef",
   "metadata": {},
   "source": [
    "### Multiple Prompt One after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a77ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Wrap it in LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe) \n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine}. Suggest fancy name for this\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest a few menu items for restaurant {restaurant_name}. return them as csv\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b92e8af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tando'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_items_chain.run(restaurant_name=name_chain.run(cuisine='Indian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dab290e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tandoori chicken tando\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "response = chain.run('Indian')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154e963",
   "metadata": {},
   "source": [
    "#### Get output from both the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5110a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model =AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Wrap it in LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe) \n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine}. Suggest fancy name for this\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key = 'restaurant_name')\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest a few menu items for restaurant {restaurant_name}. return them as csv\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key='menu_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66b061d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Mexican', 'restaurant_name': 'chile relleno', 'menu_items': 'chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile relleno chile '}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', 'menu_items']\n",
    ")\n",
    "\n",
    "response = chain({'cuisine':'Mexican'})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
